{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2B: Data Streaming Solution via BigTable\n",
    "\n",
    "## Question\n",
    "\n",
    "(b) Now, assume we needed to make a similar call, but retrieving information about many locations and much more than just the temperature. Assume it will retrieve 500MB of data a day. Describe any alterations you would want to make to your code, where the code would run each day, where the data would be saved, and which database you'd prefer to store it in.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution\n",
    "\n",
    "### Data Storage\n",
    "\n",
    "#### BigTable or Apache Hbase\n",
    "\n",
    "Writing to a csv file at scale with asychronous calls is not difficult to manage.  File corruption, migrations, and file management require admins to oversee.  \n",
    "\n",
    "A BigTable solution reduces the complexity of the data pipeline and ensures that files are never lost or mismanaged.   Instead of writing to a csv, I would prefer to write the data of each request to a big table.  \n",
    "\n",
    "If necessary, according to the workflow I would then migrate that data to a postgresql db for permanent storage.  \n",
    "\n",
    "Typically, I would run a google cloud environment to achieve this.  \n",
    "\n",
    "#### CSV Method\n",
    "\n",
    "If the csv method is the only option available, I would not expose the programs to the internet to avoid asynchronous calls.  I would also limit cpu usage to 1 thread at a time, to avoid file corruption.  \n",
    "\n",
    "I would use google cloud compute or a comparible service to run the csv solution code from my solution to 2a. It would take input from a local file containing search cities and intervals.  Each city's data will be stored on individual csv partitioned by the hour at maximum.  \n",
    "\n",
    "That data would be store do on the storage solution of the cloud platform just on google drive or dropbox via their apis if necessary.  \n",
    "\n",
    "Finally, I would migrate the data to a postgresql db(open source-no licensing fees) daily with chron jobs and schema building pyspark scripts.  I could also use also write sql queries to integrate the csv's and clean the data if the server workload is low.  \n",
    "\n",
    "### Searching for Many Cities\n",
    "\n",
    "To search for many cities, I would integrate the code below into a restful framework that passes a city name to the open weather endpoint.  \n",
    "\n",
    "It would then pass the result needed to thebig-query endpoint detailed in the part.  \n",
    "\n",
    "Finally, I would write another program that takes input from whatever source the team is most comfortable editing.  For example,  a google sheet with cities and search intervals.  Then, have the program search for data at a the interval set in the config file.   \n",
    "\n",
    "\n",
    "## The Code Below\n",
    "The code below is written to take advantage of the bigtable/hbase method.  It will run asynchronously without corrupting csv files. It also forwards the data to a rest api that I wrote to store data on a bigtable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pprint import pprint\n",
    "from pandas import json_normalize\n",
    "from flatten_json import flatten_json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from ratelimit import limits, RateLimitException, sleep_and_retry\n",
    "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
    "\n",
    "ONE_MINUTE = 60\n",
    "MAX_CALLS_PER_MINUTE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(key_path):\n",
    "    with open(key_path, \"r\") as f:\n",
    "        key = f.read()\n",
    "        key = key.rstrip('\\n')\n",
    "        pprint(key)\n",
    "    return key\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Weather Query String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_query_string(base_url,lat,long,key):\n",
    "    lat_string = \"lat={}\".format(lat)\n",
    "    long_string = \"lon={}\".format(long)\n",
    "    appid_string = \"appid={}\".format(key)\n",
    "    query_string = \"&\".join([lat_string,long_string,appid_string])\n",
    "    query_string = \"?\".join([base_url,query_string])\n",
    "    return query_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Request function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(query_string):\n",
    "    response = requests.get(query_string)\n",
    "    return response\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Json into Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_from_response(response):\n",
    "    try: \n",
    "        return response.json()\n",
    "    except:\n",
    "        raise\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Time Stamp for Unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_stamp(response_dict):\n",
    "    response_dict['timestamp'] = stamp = datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S:%f\")\n",
    "    pprint(stamp)\n",
    "    return response_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Update String to Send to my REST Server\n",
    "\n",
    "note that this function could be simplified and made dynamic, but it serves a single purpose in this instance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_update_string(response_dict):\n",
    "    #pprint(response_dict)\n",
    "\n",
    "    latitude = response_dict['coord']['lat']\n",
    "    longitude = response_dict['coord']['lon']\n",
    "    temp = response_dict['main']['temp']\n",
    "    temp_max = response_dict['main']['temp_max']\n",
    "    temp_min = response_dict['main']['temp_min']\n",
    "    humidity = response_dict['main']['humidity']\n",
    "    pressure = response_dict['main']['pressure']\n",
    "    feels_like = response_dict['main']['feels_like']\n",
    "    sunrise = response_dict['sys']['sunrise']\n",
    "    sunset =  response_dict['sys']['sunset']\n",
    "    datetime = response_dict['timestamp']\n",
    "    timezone = response_dict['timezone']\n",
    "    city_name = response_dict['name']\n",
    "    city_name = city_name.replace(\" \", \"\")\n",
    "    city_name = city_name.replace(\",\", \"-\")\n",
    "\n",
    "\n",
    "    base_url = \"http://0.0.0.0:8080/weather?\"\n",
    "    lat_str = \"lat={}\".format(str(latitude))\n",
    "    long_str = \"long={}\".format(str(longitude))\n",
    "    temp_str = \"temp={}\".format(str(temp))\n",
    "    temp_max_str = \"temp_max={}\".format(str(temp_max))\n",
    "    temp_min_str = \"temp_min={}\".format(str(temp_min))\n",
    "    feels_like_str = \"feels_like={}\".format(str(feels_like))\n",
    "    humidity_str = \"humidity={}\".format(str(humidity))\n",
    "    pressure_str = \"pressure={}\".format(str(pressure))\n",
    "    sunrise_str = \"sunrise={}\".format(str(sunrise))\n",
    "    sunset_str = \"sunset={}\".format(str(sunset))\n",
    "    datetime_str = \"datetime={}\".format(str(datetime))\n",
    "    timezone_str = \"timezone={}\".format(str(timezone))\n",
    "    city_name_str = \"city={}\".format(str(city_name))\n",
    "    \n",
    "    query_str = \"&\".join([lat_str,long_str,temp_str, temp_max_str, temp_min_str, feels_like_str, humidity_str, pressure_str,sunrise_str, sunset_str, datetime_str, timezone_str, city_name_str])\n",
    "    query_str = base_url + query_str\n",
    "    return query_str\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sleep_and_retry\n",
    "@limits(calls=MAX_CALLS_PER_MINUTE, period=ONE_MINUTE)\n",
    "def update_pipeline(count = 0):\n",
    "    #outpath = os.getcwd()\n",
    "    key_path = \"/Users/jnapolitano/Projects/pmc-submission/jupyter-book/notebooks/weather_key.txt\"\n",
    "    lat = 34.047470\n",
    "    long = -118.445950\n",
    "    key = get_key(key_path)\n",
    "    base_url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    query_string = make_query_string(base_url,lat,long,key)\n",
    "    #pprint(query_string)\n",
    "    response = make_request(query_string=query_string)\n",
    "    response_dict = make_dict_from_response(response=response)\n",
    "    response_dict = add_date_stamp(response_dict)\n",
    "    update_str = make_update_string(response_dict)\n",
    "    big_query_response = make_request(update_str)\n",
    "    pprint(big_query_response)\n",
    "    pprint(update_str)\n",
    "    pprint(count)\n",
    "\n",
    "    ## The Pool Executor is testing for thread safety.  For intance, it is running the update_pipeline 3 times at 3 different rates according\n",
    "    #the limits set by the ratelimitter.   With an exposed rest api and big table it is in theory possible to query as many times as they're are cpu cores available on the machine.  \n",
    "    with PoolExecutor(max_workers=3) as executor:\n",
    "        for _ in executor.map(update_pipeline, range(60)):\n",
    "            pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def main():\n",
    "    update_pipeline()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}