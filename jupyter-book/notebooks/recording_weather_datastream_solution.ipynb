{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2b: Data Streaming Solution via BigTable\n",
    "\n",
    "## Question\n",
    "\n",
    "(b) Now, assume we needed to make a similar call, but retrieving information about many locations and much more than just the temperature. Assume it will retrieve 500MB of data a day. Describe any alterations you would want to make to your code, where the code would run each day, where the data would be saved, and which database you'd prefer to store it in.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution\n",
    "\n",
    "### Data Storage\n",
    "\n",
    "#### BigTable or Apache Hbase\n",
    "\n",
    "Writing to a csv file at scale with asychronous calls is difficult to manage.  File corruption, migrations, and file management are problems that will manifest at scale.\n",
    "\n",
    "A BigTable solution reduces the complexity of the data pipeline and ensures that files are never lost or mismanaged.   Instead of writing to a csv, I would prefer to write the data of each request to a big table.  \n",
    "\n",
    "According to the workflow I would then migrate that data to a postgresql db for permanent storage.  \n",
    "\n",
    "Typically, I would run a google cloud environment to achieve this.  \n",
    "\n",
    "#### CSV Method\n",
    "\n",
    "If the csv method is the only option available, I would not expose the programs to the internet to avoid asynchronous calls.  I would also limit cpu usage to 1 thread at a time, to avoid file corruption.  \n",
    "\n",
    "I would use google cloud compute or a comparible service to run the csv solution code from my solution to 2a. It would take input from a local file containing search cities and intervals.  Each city's data will be stored on individual csv partitioned by the hour at maximum.  \n",
    "\n",
    "That data would be store do on the storage solution of the cloud platform.\n",
    "\n",
    "Finally, I would migrate the data to a postgresql db (open source-no licensing fees) daily with chron jobs and schema building pyspark scripts.  I could also use also write sql queries to integrate the csv's and to clean the data if the server workload is low.  \n",
    "\n",
    "### Searching for Many Cities\n",
    "\n",
    "To search for many cities, I would integrate the code below into a restful framework that passes a city name to the open weather endpoint.  \n",
    "\n",
    "It would then pass the result needed to the big-query endpoint detailed in the next section.  \n",
    "\n",
    "Finally, I would write another set of programs to run the application.   The service would take input from whatever source the team is most comfortable editing.  For example,  a google sheet with cities and search interva.  The service would make the request via an api to the solution presented below.  \n",
    "\n",
    "Each service would be containerized in order to scale at need.  Kubernetes clusters would be orchestrated to meet compute and request demands variable to reduce overall cost cost to the organization. At enterprise scale it would likely be more cost efficient to externalize these processes via the cloud.  Local hardware would be cost prohibitive.  \n",
    "\n",
    "\n",
    "## The Code Below\n",
    "\n",
    "The code below is written to take advantage of the bigtable/hbase method.  It will run asynchronously without corrupting csv files. It also forwards the data to a rest api that I wrote to store data on a bigtable.\n",
    "\n",
    "For testing, I ran the code below on my local RHEL server. It has succesfully run for three days without error.   With variation to fit the needs of another rest api, the code could be put into production within a week.  \n",
    "\n",
    "A django based web app, or preferably a go based rest api could also be produced within about two to three weeks to accomplish enterprise scale data migration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pprint import pprint\n",
    "from pandas import json_normalize\n",
    "from flatten_json import flatten_json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from ratelimit import limits, RateLimitException, sleep_and_retry\n",
    "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
    "\n",
    "ONE_MINUTE = 60\n",
    "MAX_CALLS_PER_MINUTE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(key_path):\n",
    "    with open(key_path, \"r\") as f:\n",
    "        key = f.read()\n",
    "        key = key.rstrip('\\n')\n",
    "        pprint(key)\n",
    "    return key\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Weather Query String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_query_string(base_url,lat,long,key):\n",
    "    lat_string = \"lat={}\".format(lat)\n",
    "    long_string = \"lon={}\".format(long)\n",
    "    appid_string = \"appid={}\".format(key)\n",
    "    query_string = \"&\".join([lat_string,long_string,appid_string])\n",
    "    query_string = \"?\".join([base_url,query_string])\n",
    "    return query_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Request function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(query_string):\n",
    "    response = requests.get(query_string)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Json into Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_from_response(response):\n",
    "    try: \n",
    "        return response.json()\n",
    "    except:\n",
    "        raise\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Time Stamp for Unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_stamp(response_dict):\n",
    "    response_dict['timestamp'] = stamp = datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S:%f\")\n",
    "    pprint(stamp)\n",
    "    return response_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Update String to Send to my REST Server\n",
    "\n",
    "note that this function could be simplified and made dynamic, but it serves a single purpose in this instance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_update_string(response_dict):\n",
    "    #pprint(response_dict)\n",
    "\n",
    "    latitude = response_dict['coord']['lat']\n",
    "    longitude = response_dict['coord']['lon']\n",
    "    temp = response_dict['main']['temp']\n",
    "    temp_max = response_dict['main']['temp_max']\n",
    "    temp_min = response_dict['main']['temp_min']\n",
    "    humidity = response_dict['main']['humidity']\n",
    "    pressure = response_dict['main']['pressure']\n",
    "    feels_like = response_dict['main']['feels_like']\n",
    "    sunrise = response_dict['sys']['sunrise']\n",
    "    sunset =  response_dict['sys']['sunset']\n",
    "    datetime = response_dict['timestamp']\n",
    "    timezone = response_dict['timezone']\n",
    "    city_name = response_dict['name']\n",
    "    city_name = city_name.replace(\" \", \"\")\n",
    "    city_name = city_name.replace(\",\", \"-\")\n",
    "\n",
    "\n",
    "    base_url = \"http://0.0.0.0:8080/weather?\"\n",
    "    lat_str = \"lat={}\".format(str(latitude))\n",
    "    long_str = \"long={}\".format(str(longitude))\n",
    "    temp_str = \"temp={}\".format(str(temp))\n",
    "    temp_max_str = \"temp_max={}\".format(str(temp_max))\n",
    "    temp_min_str = \"temp_min={}\".format(str(temp_min))\n",
    "    feels_like_str = \"feels_like={}\".format(str(feels_like))\n",
    "    humidity_str = \"humidity={}\".format(str(humidity))\n",
    "    pressure_str = \"pressure={}\".format(str(pressure))\n",
    "    sunrise_str = \"sunrise={}\".format(str(sunrise))\n",
    "    sunset_str = \"sunset={}\".format(str(sunset))\n",
    "    datetime_str = \"datetime={}\".format(str(datetime))\n",
    "    timezone_str = \"timezone={}\".format(str(timezone))\n",
    "    city_name_str = \"city={}\".format(str(city_name))\n",
    "    \n",
    "    query_str = \"&\".join([lat_str,long_str,temp_str, temp_max_str, temp_min_str, feels_like_str, humidity_str, pressure_str,sunrise_str, sunset_str, datetime_str, timezone_str, city_name_str])\n",
    "    query_str = base_url + query_str\n",
    "    return query_str\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sleep_and_retry\n",
    "@limits(calls=MAX_CALLS_PER_MINUTE, period=ONE_MINUTE)\n",
    "def update_pipeline(count = 0):\n",
    "    #outpath = os.getcwd()\n",
    "    key_path = \"/Users/jnapolitano/Projects/pmc-submission/jupyter-book/notebooks/weather_key.txt\"\n",
    "    lat = 34.047470\n",
    "    long = -118.445950\n",
    "    key = get_key(key_path)\n",
    "    base_url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    query_string = make_query_string(base_url,lat,long,key)\n",
    "    #pprint(query_string)\n",
    "    response = make_request(query_string=query_string)\n",
    "    response_dict = make_dict_from_response(response=response)\n",
    "    response_dict = add_date_stamp(response_dict)\n",
    "    update_str = make_update_string(response_dict)\n",
    "    big_query_response = make_request(update_str)\n",
    "    pprint(big_query_response)\n",
    "    pprint(update_str)\n",
    "    pprint(count)\n",
    "\n",
    "    ## The Pool Executor is testing for thread safety.  For intance, it is running the update_pipeline 3 times at 3 different rates according\n",
    "    #the limits set by the ratelimitter.   With an exposed rest api and big table it is in theory possible to query as many times as they're are cpu cores available on the machine.  \n",
    "    with PoolExecutor(max_workers=3) as executor:\n",
    "        for _ in executor.map(update_pipeline, range(60)):\n",
    "            pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def main():\n",
    "    update_pipeline()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
